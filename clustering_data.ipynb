{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c0c4df-ab90-44b8-aade-489a12bf9d77",
   "metadata": {},
   "source": [
    "## Cell 1: Importing Libraries, Dataset, and Using EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b5acb3-7aa8-402d-ae2b-fe394e30d5a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'file_goes_here.csv'  # Replace with the path to the CSV file as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922e8e5-136d-4a5b-8da2-59395e487cdb",
   "metadata": {},
   "source": [
    "## Cell 2: Data Preparation for Clustering\n",
    "\n",
    "In this section, we prepare our dataset for the clustering analysis. The dataset consists of students' academic scores (`Semester_Total`) and their attendance records (`Absence_Percentage`). \n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "We start by extracting the relevant features for clustering:\n",
    "\n",
    "- `Semester_Total`: The total academic score for the semester, which may reflect the students' academic performance.\n",
    "- `Absence_Percentage`: The percentage of classes the student has missed, which can be an indicator of engagement or external factors affecting the student's ability to attend.\n",
    "\n",
    "These two features are chosen because they are likely to provide meaningful insights when distinguishing between different groups of students.\n",
    "\n",
    "### Standardization\n",
    "\n",
    "To ensure that each feature contributes equally to the distance calculations during clustering, we standardize the features using `StandardScaler`. This scaling technique transforms our data such that the distribution of each feature has a mean value of 0 and a standard deviation of 1. In mathematical terms, for each feature:\n",
    "\n",
    "- The mean (μ) is subtracted from each data point.\n",
    "- The result is then divided by the standard deviation (σ).\n",
    "\n",
    "This process is crucial because the K-means clustering algorithm, which we intend to use, is sensitive to the scales of the data points and relies on the Euclidean distance between them. You can find the results of these two important values after the graphical representation of the `Data after Scaling`.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "After scaling, we visualize the standardized features using a scatter plot. This gives us a preliminary look at our data in the standardized feature space and may reveal any apparent groupings or outliers that exist before we proceed with the clustering algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326c4fb-c8dc-43a5-b6ce-02d708ec34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selecting the relevant features for clustering\n",
    "X = df[['Semester_Total', 'Absence_Percentage']].values\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Visualizing the data (Optional)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], s=50)\n",
    "plt.title(\"Data after Scaling\")\n",
    "plt.xlabel('Semester Total (Standardized)')\n",
    "plt.ylabel('Absence Percentage (Standardized)')\n",
    "plt.show()\n",
    "\n",
    "# Print the mean and standard deviation used for scaling\n",
    "print(\"Feature Means:\", scaler.mean_)\n",
    "print(\"Feature Standard Deviations:\", scaler.scale_)\n",
    "\n",
    "# Create a DataFrame from the scaled features\n",
    "scaled_df = pd.DataFrame(X_scaled, columns=['Semester_Total_Scaled', 'Absence_Percentage_Scaled'])\n",
    "\n",
    "# Add columns for the mean and standard deviation\n",
    "scaled_df['Semester_Total_Mean'] = scaler.mean_[0]\n",
    "scaled_df['Semester_Total_Std'] = scaler.scale_[0]\n",
    "scaled_df['Absence_Percentage_Mean'] = scaler.mean_[1]\n",
    "scaled_df['Absence_Percentage_Std'] = scaler.scale_[1]\n",
    "\n",
    "# Export to CSV\n",
    "scaled_df.to_csv('scaled_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9407e7-90d6-4cb4-83ef-74ba789dc0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "data_path = 'scaled_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48e07e-0102-4521-81b8-0221e471ffbe",
   "metadata": {},
   "source": [
    "### Breakdown of Each Column\n",
    "`Semester_Total_Scaled`: This column contains the scaled values of the `Semester_Total` feature. Scaling is a method used to standardize the range of independent variables or features of data. In this case, it seems like standard scaling has been used, which means the feature has been scaled to have a mean of 0 and a standard deviation of 1. The actual values are the number of standard deviations away from the mean each original value was.\n",
    "\n",
    "`Absence_Percentage_Scaled`: Similar to the `Semester_Total_Scaled` column, this represents the scaled values of the `Absence_Percentage` feature. The scaling process normalizes the distribution of the data, allowing for a more meaningful comparison between the variables.\n",
    "\n",
    "`Semester_Total_Mean`: This is the mean of the `Semester_Total` feature before it was scaled. All the values in this column are the same, indicating that the mean value was calculated from the original data and then used to scale the `Semester_Total` feature.\n",
    "\n",
    "`Semester_Total_Std`: This column shows the standard deviation of the `Semester_Total` feature before scaling. It's used alongside the mean to scale the data. The standard deviation is a measure of the amount of variation or dispersion in a set of values. A low standard deviation indicates that the values tend to be close to the mean, while a high standard deviation indicates that the values are spread out over a wider range.\n",
    "\n",
    "`Absence_Percentage_Mean`: This is the mean of the `Absence_Percentage` feature before it was scaled. Similar to the `Semester_Total_Mean,` it's constant across all rows, indicating it was computed from the entire dataset before scaling.\n",
    "\n",
    "`Absence_Percentage_Std`: This column shows the standard deviation of the `Absence_Percentage` feature before it was scaled. Like the `Semester_Total_Std`, it's a measure of how much the absence percentages vary from the average absence percentage.\n",
    "\n",
    "### What's Happening with the Data?\n",
    "**Standardization**: The original `Semester_Total` and `Absence_Percentage` features have been standardized. Standardization typically transforms data to have a mean of 0 and a standard deviation of 1. This process makes the data unitless and brings different variables to a comparable scale, making it easier to apply machine learning algorithms effectively.\n",
    "\n",
    "**Record of Parameters**: The mean and standard deviation for each feature before scaling are recorded. This is crucial for several reasons:\n",
    "* Interpretability: You can understand how the data was transformed and interpret the scaled values in the context of the original data distribution.\n",
    "* Consistency: If you want to apply the same transformation to new data (e.g., during a model deployment phase), you'll need these parameters to ensure that the scaling is consistent with the original model training data.\n",
    "* Reversibility: If you need to convert the scaled data back to the original scale for interpretation or reporting, these values will be necessary to reverse the scaling.\n",
    "\n",
    "### Why is this Useful?\n",
    "1. Model Readiness: Many machine learning algorithms perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed.\n",
    "2. Outlier Sensitivity Reduction: Algorithms that are sensitive to the scale of the data, like K-Nearest Neighbors, or models that use distance measures, like K-Means clustering, can perform better if the scale of the data doesn't overly influence the feature representation.\n",
    "3. Improved Learning: Features with large ranges can disproportionately influence neural networks and gradient descent algorithms. Scaling can mitigate this issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1a124-e5c1-4007-855a-df6753194db1",
   "metadata": {},
   "source": [
    "## Cell 3: Using the Elbow Method & Silhouette Scores to Calculate the Optimal Number of Clusters\n",
    "\n",
    "### Choose a Range for K\n",
    "\n",
    "`K_range = range(1, 11)` is a Python command that creates a range object representing a sequence of numbers from 1 to 10 (inclusive of 1 and exclusive of 11). This is often used in the context of K-means clustering or other K-related algorithms where you might want to iterate over several values of K to determine the optimal number.\n",
    "\n",
    "**Components:**\n",
    "* `range(start, stop)`: This is a built-in Python function that generates a sequence of numbers. range(1, 11) generates numbers from 1 to 10.\n",
    "* `K_range`: This is the variable name where the range object is stored. The name suggests it's being used to store a range of values for \"K\".\n",
    "\n",
    "**Uses in Clustering (K-means):**\n",
    "\n",
    "Determining Optimal K: In K-means clustering, \"K\" represents the number of clusters you want to divide your data into. However, the optimal number of clusters is not always apparent and often needs to be determined empirically. A common method for finding the optimal K is the Elbow Method, where you run K-means for a range of K values and then plot the results to look for an \"elbow\" point where the rate of decrease sharply changes. This point is often considered a good trade-off for the number of clusters.\n",
    "\n",
    "Iterating Through K Values: The variable K_range can be used in a for loop to systematically apply K-means clustering (or other K-related algorithms) with different K values. For each K, you might calculate metrics (e.g., Within-Cluster-Sum of Squared Errors (WSS), Silhouette Score) to assess the quality of the clustering.\n",
    "\n",
    "### Run K-means for Each K and Calculate Inertia\n",
    "\n",
    "This cell's code block is part of a process used to find the best number of groups, or \"clusters\", for organizing a dataset based on similarities among the data points:\n",
    "\n",
    "- **Tracking Performance**: It initializes an empty list named `inertias`, which will be used to record a score that indicates how well the data points within each cluster are grouped.\n",
    "\n",
    "- **Testing Cluster Counts**: It then sets up a loop to test different numbers of clusters, ranging from 1 to a maximum number (this maximum is determined by `K_range`).\n",
    "\n",
    "- **Building Clusters**: For each number in that range, it creates a new cluster model and applies it to the data that's been scaled to a uniform size. The `n_init` parameter is set to 5, meaning it will try 5 different starting points and choose the best result for each number of clusters.\n",
    "\n",
    "- **Evaluating the Clusters**: After the model has created the clusters, the code calculates the 'inertia' for each model — this is a measure of how internally coherent the clusters are.\n",
    "\n",
    "- **Recording Results**: The calculated inertia is added to the list created earlier. Once the loop is done, the code prints out the list of inertias.\n",
    "\n",
    "The inertias give us a clue about the best number of clusters by showing us how much each additional cluster improves the tightness of the clustering. Generally, we look for a point where adding more clusters doesn't improve the inertia by much, known as the \"elbow\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183a60f-0562-4f2d-bd21-ae3578ced91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_range = range(1, 11)\n",
    "\n",
    "inertias = []\n",
    "\n",
    "for k in K_range:\n",
    "    # Create a KMeans instance with k clusters and explicitly set n_init to 5\n",
    "    kmeans = KMeans(n_clusters=k, n_init=5, random_state=42)\n",
    "    \n",
    "    # Fit the model to the scaled data\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Print the inertias for review\n",
    "print(inertias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c63bf-76f1-4147-86a6-2d717b80d74e",
   "metadata": {},
   "source": [
    "### Plot the Elbow Curve\n",
    "\n",
    "If the code doesn't automatically determine the elbow point, then you should observe the plot and choose the point where the inertia begins to decrease more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a9c47-eda0-457c-969f-2342523a3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters, k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(K_range)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff229e73-8fb0-4eda-84e4-cc576338af6b",
   "metadata": {},
   "source": [
    "### Calculate the Silhouette Score (Higher is Better)\n",
    "\n",
    "This code block is part of a data analysis pipeline that reads a dataset, selects specific features for clustering, scales those features, applies K-means clustering, and then evaluates the clustering result using the Silhouette Score.\n",
    "\n",
    "#### When or If Changes Need to Be Done\n",
    "\n",
    "If you want to experiment with the number of clusters (`n_clusters`), you would change the value from 5 to the desired number of clusters.\n",
    "\n",
    "If you want to ensure the stability of the results, you might change `n_init` to a higher value to perform more runs with different centroid seeds.\n",
    "\n",
    "The `random_state` can be adjusted or removed if you want different results each time for comparison.\n",
    "If you decide to use a different set of features for clustering, you would modify the X DataFrame to include those columns.\n",
    "\n",
    "If the Silhouette Score is low, it may suggest that the chosen number of clusters is not ideal, and you might consider adjusting `n_clusters` or revisiting your feature selection or preprocessing steps.\n",
    "\n",
    "At the end, the exact coordinates of the cluster centers are printed using the `kmeans.cluster_centers_` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5c075-65e8-4d28-9d31-d61d3ba0db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reads the original dataset path\n",
    "data_path = 'applied_college.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# These are the columns to be used for clustering\n",
    "X = df[['Semester_Total', 'Absence_Percentage']]\n",
    "\n",
    "# Scale the features before clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply K-means clustering (n_init tells how many times to run the clustering with different centroid seeds)\n",
    "kmeans = KMeans(n_clusters=5, n_init=20, random_state=0).fit(X_scaled)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)\n",
    "\n",
    "# Print the Silhouette Score (Higher is better)\n",
    "print(f\"The Silhouette Score for the clustering is: {silhouette_avg}\")\n",
    "print(f\"\\n Cluster centers:\")\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7f9ce-c7b4-4609-a76f-697782df48f3",
   "metadata": {},
   "source": [
    "## Cell 4: Plotting the Results\n",
    "\n",
    "This cell is used to visualize data that has been organized into groups, or \"clusters\", based on similarity. Here's what each part does:\n",
    "\n",
    "- **Setting Up the Plot**: It prepares a visual space, setting the size of the plot so the data will be easy to see.\n",
    "\n",
    "- **Displaying the Data**: It creates a scatter plot, which is a type of graph that shows individual data points on an X-Y axis. The data points represent two specific characteristics of the data — for this example, these are the 'Semester_Total' and 'Absence_Percentage' after they've been scaled to a standard size. Each point is colored based on the cluster it belongs to, showing the grouping created by the clustering process.\n",
    "\n",
    "- **Highlighting the Centers**: The code also marks the \"center\" of each cluster with a special symbol (an 'X') and a different color. These centers are like the average location for each group, showing the middle ground of all the points in that cluster.\n",
    "\n",
    "- **Adding Details**: To make the graph more informative, it includes a title, labels for each axis, and a color legend that explains the color coding of the clusters.\n",
    "\n",
    "- **Presenting the Plot**: Finally, the graph is displayed to the user, allowing them to see the distribution of the data and how it's been clustered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc9fe8-2213-4c95-994d-5787dd5e4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scatter plot of the two features, color-coded by cluster label\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans.labels_, cmap='viridis', marker='o', edgecolor='k')\n",
    "\n",
    "# Plot the centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "\n",
    "# Optional enhancements\n",
    "plt.title('Clustered Data Points')\n",
    "plt.xlabel('Semester_Total (scaled)')\n",
    "plt.ylabel('Absence_Percentage (scaled)')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbcbd45-0c27-42b3-921f-28b35d5baa15",
   "metadata": {},
   "source": [
    "## Points to Consider when Categorizing Clusters\n",
    "\n",
    "1. Demographics: Age, gender, nationality, and socioeconomic status could reveal patterns in attendance and performance.\n",
    "2. Academic Performance: Average grades, major subjects, and performance trends over time. For example, performance in core vs. elective courses.\n",
    "3. Attendance Patterns: Frequency and timing of absences (e.g., more absences around certain events or times of the semester). For example, correlation between attendance and exam periods or deadlines.\n",
    "4. Engagement: Participation in class, such as involvement in discussions, group work, and other interactive segments of the course. Engagement in extracurricular activities, which could be an indicator of overall engagement with the college experience.\n",
    "5. Financial Status: Scholarship status, need for financial aid, or employment alongside studies, which might impact both attendance and performance.\n",
    "6. Housing and Commute: Whether students live on-campus, off-campus, or commute from home, and how this might impact their attendance and performance.\n",
    "7. Health and Well-being: Access to and use of health services, and any reported health issues that might affect attendance. Mental health considerations, particularly stress or anxiety related to studies or personal life.\n",
    "8. Study Habits: Time spent on self-study, group study sessions, or utilization of academic support services such as tutoring. For example, preferred study times and methods (e.g., late-night studying vs. regular hours, digital vs. traditional materials).\n",
    "9. Technology Access: Access to and usage of technology for learning, which could influence study habits and academic performance.\n",
    "10. Language Proficiency: Proficiency in the language of instruction, which in many Middle Eastern universities is often English, and how this impacts academic performance.\n",
    "11. Cultural Factors: Cultural obligations, such as family responsibilities or religious practices, that might influence attendance and study patterns.\n",
    "12. Feedback from Instructors: Instructors' observations about students' behavior, participation, and any challenges they might be facing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf60c7-17c8-499a-80c9-d7325a7fc489",
   "metadata": {},
   "source": [
    "This Python notebook was created by S. Hatting for the University of Tabuk's English Language Institute in January 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
